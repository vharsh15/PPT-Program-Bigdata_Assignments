{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b79ddb",
   "metadata": {},
   "source": [
    "1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a51200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "def display_core_components():\n",
    "    # Create a ConfigParser object\n",
    "    config = configparser.ConfigParser()\n",
    "\n",
    "    # Read the Hadoop configuration file\n",
    "    config.read('/path/to/hadoop/conf/core-site.xml')\n",
    "\n",
    "    # Get the value of the property \"fs.defaultFS\"\n",
    "    default_fs = config.get('core-site', 'fs.defaultFS')\n",
    "\n",
    "    # Get the value of the property \"mapreduce.framework.name\"\n",
    "    framework_name = config.get('core-site', 'mapreduce.framework.name')\n",
    "\n",
    "    # Get the value of the property \"yarn.resourcemanager.address\"\n",
    "    resource_manager_address = config.get('yarn-site', 'yarn.resourcemanager.address')\n",
    "\n",
    "    # Display the core components\n",
    "    print('Default File System:', default_fs)\n",
    "    print('MapReduce Framework:', framework_name)\n",
    "    print('Resource Manager Address:', resource_manager_address)\n",
    "\n",
    "# Call the function to display the core components\n",
    "display_core_components()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7fe85c",
   "metadata": {},
   "source": [
    "2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7faa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def calculate_total_file_size(hdfs_url, hdfs_directory):\n",
    "    # Create an HDFS client\n",
    "    client = InsecureClient(hdfs_url)\n",
    "\n",
    "    # Get the file status of the directory\n",
    "    dir_status = client.status(hdfs_directory)\n",
    "\n",
    "    # Initialize total file size\n",
    "    total_size = 0\n",
    "\n",
    "    # Recursively traverse the directory\n",
    "    for root, dirs, files in client.walk(hdfs_directory):\n",
    "        # Calculate the size of each file and add it to the total\n",
    "        for file in files:\n",
    "            file_path = f\"{root}/{file}\"\n",
    "            file_status = client.status(file_path)\n",
    "            total_size += file_status['length']\n",
    "\n",
    "    # Convert total_size to human-readable format\n",
    "    total_size_mb = total_size / (1024 * 1024)\n",
    "\n",
    "    # Return the total file size\n",
    "    return total_size_mb\n",
    "\n",
    "# Example usage\n",
    "hdfs_url = 'http://localhost:9870'  # Replace with your HDFS URL\n",
    "hdfs_directory = '/user/example'  # Replace with your HDFS directory\n",
    "total_size = calculate_total_file_size(hdfs_url, hdfs_directory)\n",
    "print(f\"Total File Size: {total_size:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d83dbf",
   "metadata": {},
   "source": [
    "3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aef0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "class MRWordFrequency(MRJob):\n",
    "    \n",
    "    def mapper_get_words(self, _, line):\n",
    "        # Split the line into words using regular expression\n",
    "        words = re.findall(r'\\w+', line.lower())\n",
    "        \n",
    "        # Emit each word as a key with count as 1\n",
    "        for word in words:\n",
    "            yield word, 1\n",
    "    \n",
    "    def combiner_count_words(self, word, counts):\n",
    "        # Sum the counts of each word emitted by the mapper\n",
    "        yield word, sum(counts)\n",
    "    \n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # Sum the counts of each word emitted by the combiner\n",
    "        yield word, sum(counts)\n",
    "    \n",
    "    def mapper_sort_words(self, word, count):\n",
    "        # Swap the key-value pairs for sorting\n",
    "        yield None, (count, word)\n",
    "    \n",
    "    def reducer_sort_words(self, _, count_word_pairs):\n",
    "        # Sort the word-count pairs in descending order of count\n",
    "        sorted_pairs = sorted(count_word_pairs, reverse=True)\n",
    "        \n",
    "        # Yield the top N word-count pairs\n",
    "        for i in range(N):\n",
    "            count, word = sorted_pairs[i]\n",
    "            yield word, count\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(mapper=self.mapper_sort_words,\n",
    "                   reducer=self.reducer_sort_words)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    N = 10  # Number of top words to display\n",
    "    input_file = 'path/to/large/text/file.txt'  # Replace with the path to your large text file\n",
    "    \n",
    "    # Run the MapReduce job\n",
    "    MRWordFrequency.run(['-r', 'local', input_file])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf2b32",
   "metadata": {},
   "source": [
    "4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73efe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def check_namenode_health(namenode_url):\n",
    "    # Send a GET request to the NameNode's JMX endpoint\n",
    "    jmx_url = f\"{namenode_url}/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo\"\n",
    "    response = requests.get(jmx_url)\n",
    "    data = response.json()\n",
    "\n",
    "    # Check the health status of the NameNode\n",
    "    if \"beans\" in data:\n",
    "        for bean in data[\"beans\"]:\n",
    "            if \"NNRole\" in bean and bean[\"NNRole\"] == \"active\":\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def check_datanode_health(datanode_url):\n",
    "    # Send a GET request to the DataNode's JMX endpoint\n",
    "    jmx_url = f\"{datanode_url}/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState\"\n",
    "    response = requests.get(jmx_url)\n",
    "    data = response.json()\n",
    "\n",
    "    # Check the health status of the DataNode\n",
    "    if \"beans\" in data:\n",
    "        for bean in data[\"beans\"]:\n",
    "            if \"VolumeFailuresTotal\" in bean and bean[\"VolumeFailuresTotal\"] == 0:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    namenode_url = \"http://<namenode-hostname>:50070\"  # Replace with the URL of your NameNode\n",
    "    datanode_urls = [\n",
    "        \"http://<datanode1-hostname>:50075\",  # Replace with the URLs of your DataNodes\n",
    "        \"http://<datanode2-hostname>:50075\",\n",
    "        \"http://<datanode3-hostname>:50075\"\n",
    "    ]\n",
    "\n",
    "    # Check the health status of the NameNode\n",
    "    namenode_health = check_namenode_health(namenode_url)\n",
    "    if namenode_health:\n",
    "        print(\"NameNode is healthy\")\n",
    "    else:\n",
    "        print(\"NameNode is not healthy\")\n",
    "\n",
    "    # Check the health status of each DataNode\n",
    "    for datanode_url in datanode_urls:\n",
    "        datanode_health = check_datanode_health(datanode_url)\n",
    "        if datanode_health:\n",
    "            print(f\"DataNode at {datanode_url} is healthy\")\n",
    "        else:\n",
    "            print(f\"DataNode at {datanode_url} is not healthy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2eff92",
   "metadata": {},
   "source": [
    "5. Develop a Python program that lists all the files and directories in a specific HDFS path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3545fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def list_hdfs_path(hdfs_url, hdfs_path):\n",
    "    # Create an HDFS client\n",
    "    client = InsecureClient(hdfs_url)\n",
    "\n",
    "    # List all files and directories in the HDFS path\n",
    "    contents = client.list(hdfs_path, status=True)\n",
    "\n",
    "    # Print the list of files and directories\n",
    "    for item in contents:\n",
    "        if item['type'] == 'DIRECTORY':\n",
    "            print('Directory:', item['path'])\n",
    "        else:\n",
    "            print('File:', item['path'])\n",
    "\n",
    "# Example usage\n",
    "hdfs_url = 'http://localhost:9870'  # Replace with your HDFS URL\n",
    "hdfs_path = '/user/example'  # Replace with the desired HDFS path\n",
    "list_hdfs_path(hdfs_url, hdfs_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f82ef28",
   "metadata": {},
   "source": [
    "6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb63db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_datanode_info(hadoop_url):\n",
    "    # Send a GET request to the DataNodes endpoint\n",
    "    datanodes_url = f\"{hadoop_url}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
    "    response = requests.get(datanodes_url)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the DataNode information\n",
    "    if \"beans\" in data:\n",
    "        datanodes = data[\"beans\"]\n",
    "        return datanodes\n",
    "\n",
    "    return []\n",
    "\n",
    "def analyze_datanode_storage_utilization(hadoop_url):\n",
    "    # Retrieve the DataNode information\n",
    "    datanodes = get_datanode_info(hadoop_url)\n",
    "\n",
    "    if len(datanodes) > 0:\n",
    "        # Sort the DataNodes based on their storage capacities\n",
    "        sorted_datanodes = sorted(datanodes, key=lambda d: d[\"Capacity\"], reverse=True)\n",
    "\n",
    "        # Extract the DataNode with the highest storage capacity\n",
    "        highest_capacity_datanode = sorted_datanodes[0]\n",
    "        highest_capacity_hostname = highest_capacity_datanode[\"HostAndPort\"]\n",
    "\n",
    "        # Extract the DataNode with the lowest storage capacity\n",
    "        lowest_capacity_datanode = sorted_datanodes[-1]\n",
    "        lowest_capacity_hostname = lowest_capacity_datanode[\"HostAndPort\"]\n",
    "\n",
    "        # Display the results\n",
    "        print(f\"Highest storage capacity: {highest_capacity_hostname}\")\n",
    "        print(f\"Lowest storage capacity: {lowest_capacity_hostname}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No DataNode information found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hadoop_url = \"http://<namenode-hostname>:50070\"  # Replace with the URL of your NameNode\n",
    "\n",
    "    # Analyze the storage utilization of DataNodes\n",
    "    analyze_datanode_storage_utilization(hadoop_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0cf2ed",
   "metadata": {},
   "source": [
    "7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12975dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "def submit_hadoop_job(resource_manager_url, job_parameters):\n",
    "    # Construct the URL for submitting the Hadoop job\n",
    "    submit_url = f\"{resource_manager_url}/ws/v1/cluster/apps/new-application\"\n",
    "    \n",
    "    # Send a POST request to submit the Hadoop job\n",
    "    response = requests.post(submit_url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract the application ID from the response\n",
    "    application_id = data[\"application-id\"]\n",
    "\n",
    "    # Construct the URL for submitting the job request\n",
    "    submit_job_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}/app\"\n",
    "    \n",
    "    # Set the job parameters in the request payload\n",
    "    payload = {\n",
    "        \"application-id\": application_id,\n",
    "        \"application-name\": \"My Hadoop Job\",\n",
    "        \"am-container-spec\": {\n",
    "            \"commands\": {\n",
    "                \"command\": job_parameters\n",
    "            },\n",
    "            \"local-resources\": {}\n",
    "        },\n",
    "        \"application-type\": \"MAPREDUCE\"\n",
    "    }\n",
    "\n",
    "    # Send a PUT request to submit the job request\n",
    "    response = requests.put(submit_job_url, json=payload)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract the job ID from the response\n",
    "    job_id = data[\"application-id\"]\n",
    "    \n",
    "    return job_id\n",
    "\n",
    "def monitor_job_progress(resource_manager_url, job_id):\n",
    "    # Construct the URL for monitoring the job progress\n",
    "    job_status_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{job_id}\"\n",
    "    \n",
    "    while True:\n",
    "        # Send a GET request to get the job status\n",
    "        response = requests.get(job_status_url)\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract the job state from the response\n",
    "        state = data[\"app\"][\"state\"]\n",
    "        \n",
    "        # Display the job state\n",
    "        print(f\"Job ID: {job_id}, State: {state}\")\n",
    "        \n",
    "        # Check if the job is in a final state\n",
    "        if state in [\"FINISHED\", \"FAILED\", \"KILLED\"]:\n",
    "            break\n",
    "        \n",
    "        # Wait for 5 seconds before checking the status again\n",
    "        time.sleep(5)\n",
    "\n",
    "def retrieve_job_output(resource_manager_url, job_id):\n",
    "    # Construct the URL for retrieving the job output\n",
    "    job_output_url = f\"{resource_manager_url}/proxy/{job_id}/ws/v1/mapreduce/jobs/{job_id}/jobattempts\"\n",
    "    \n",
    "    # Send a GET request to retrieve the job output\n",
    "    response = requests.get(job_output_url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract the final output from the response\n",
    "    output = data[\"jobAttempts\"][\"jobAttempt\"][0][\"assignedContainerLogs\"][\"containerLogFiles\"][0][\"file\"]\n",
    "    \n",
    "    return output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    resource_manager_url = \"http://localhost:8088\"  # Replace with the URL of your ResourceManager\n",
    "    job_parameters = [\"hadoop\", \"jar\", \"/path/to/hadoop-job.jar\", \"input\", \"output\"]  # Replace with your job parameters\n",
    "    \n",
    "    # Submit the Hadoop job and retrieve the job ID\n",
    "    job_id = submit_hadoop_job(resource_manager_url, job_parameters)\n",
    "    print(f\"Submitted Hadoop job. Job ID: {job_id}\")\n",
    "    \n",
    "    # Monitor the job progress\n",
    "    monitor_job_progress(resource_manager_url, job_id)\n",
    "    \n",
    "    # Retrieve the final output\n",
    "    final_output = retrieve_job_output(resource_manager_url, job_id)\n",
    "    print(f\"Final output: {final_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2482b11a",
   "metadata": {},
   "source": [
    "8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c9274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "def submit_hadoop_job(resource_manager_url, job_parameters, memory_mb, vcores):\n",
    "    # Construct the URL for submitting the Hadoop job\n",
    "    submit_url = f\"{resource_manager_url}/ws/v1/cluster/apps/new-application\"\n",
    "\n",
    "    # Send a POST request to submit the Hadoop job\n",
    "    response = requests.post(submit_url)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the application ID from the response\n",
    "    application_id = data[\"application-id\"]\n",
    "\n",
    "    # Construct the URL for submitting the job request\n",
    "    submit_job_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}/app\"\n",
    "\n",
    "    # Set the job parameters and resource requirements in the request payload\n",
    "    payload = {\n",
    "        \"application-id\": application_id,\n",
    "        \"application-name\": \"My Hadoop Job\",\n",
    "        \"am-container-spec\": {\n",
    "            \"commands\": {\n",
    "                \"command\": job_parameters\n",
    "            },\n",
    "            \"local-resources\": {}\n",
    "        },\n",
    "        \"application-type\": \"MAPREDUCE\",\n",
    "        \"resource\": {\n",
    "            \"memory\": memory_mb,\n",
    "            \"vCores\": vcores\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Send a PUT request to submit the job request\n",
    "    response = requests.put(submit_job_url, json=payload)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the job ID from the response\n",
    "    job_id = data[\"application-id\"]\n",
    "\n",
    "    return job_id\n",
    "\n",
    "def monitor_resource_usage(resource_manager_url, job_id):\n",
    "    # Construct the URL for monitoring the resource usage\n",
    "    resource_usage_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{job_id}/appattempts\"\n",
    "\n",
    "    while True:\n",
    "        # Send a GET request to get the resource usage\n",
    "        response = requests.get(resource_usage_url)\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract the resource usage details\n",
    "        resource_attempts = data[\"appAttempts\"][\"appAttempt\"]\n",
    "        if len(resource_attempts) > 0:\n",
    "            latest_attempt = resource_attempts[-1]\n",
    "            allocated_memory_mb = latest_attempt[\"allocatedMB\"]\n",
    "            allocated_vcores = latest_attempt[\"allocatedVCores\"]\n",
    "            state = latest_attempt[\"appAttemptState\"]\n",
    "\n",
    "            # Display the resource usage details\n",
    "            print(f\"Job ID: {job_id}, State: {state}, Allocated Memory: {allocated_memory_mb} MB, Allocated vCores: {allocated_vcores}\")\n",
    "\n",
    "            # Check if the job is in a final state\n",
    "            if state in [\"FINISHED\", \"FAILED\", \"KILLED\"]:\n",
    "                break\n",
    "\n",
    "        # Wait for 5 seconds before checking the status again\n",
    "        time.sleep(5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    resource_manager_url = \"http://localhost:8088\"  # Replace with the URL of your ResourceManager\n",
    "    job_parameters = [\"hadoop\", \"jar\", \"/path/to/hadoop-job.jar\", \"input\", \"output\"]  # Replace with your job parameters\n",
    "    memory_mb = 4096  # Set the memory requirements for the job in megabytes\n",
    "    vcores = 2  # Set the number of virtual cores required for the job\n",
    "\n",
    "    # Submit the Hadoop job and retrieve the job ID\n",
    "    job_id = submit_hadoop_job(resource_manager_url, job_parameters, memory_mb, vcores)\n",
    "    print(f\"Submitted Hadoop job. Job ID: {job_id}\")\n",
    "\n",
    "    # Monitor the resource usage of the job\n",
    "    monitor_resource_usage(resource_manager_url, job_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491fbdbe",
   "metadata": {},
   "source": [
    "9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "import time\n",
    "\n",
    "class MRInputSplitSize(MRJob):\n",
    "    \n",
    "    def configure_args(self):\n",
    "        super(MRInputSplitSize, self).configure_args()\n",
    "        self.add_passthru_arg('--split-size', default=64,\n",
    "                              help='Input split size in megabytes')\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        # No actual mapping is performed for this example\n",
    "        pass\n",
    "    \n",
    "    def reducer(self, key, values):\n",
    "        # No actual reducing is performed for this example\n",
    "        pass\n",
    "    \n",
    "    def mapper_final(self):\n",
    "        # Emit the current timestamp as the mapper's final output\n",
    "        yield None, time.time()\n",
    "    \n",
    "    def reducer_final(self):\n",
    "        # Calculate and yield the execution time\n",
    "        start_time = min(self.values)\n",
    "        end_time = max(self.values)\n",
    "        execution_time = end_time - start_time\n",
    "        yield 'Execution Time', execution_time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # List of input split sizes to compare\n",
    "    split_sizes = [64, 128, 256, 512]  # In megabytes\n",
    "    \n",
    "    for split_size in split_sizes:\n",
    "        # Run the MapReduce job with the specified input split size\n",
    "        job = MRInputSplitSize(args=['--split-size', str(split_size)])\n",
    "        with job.make_runner() as runner:\n",
    "            runner.run()\n",
    "            \n",
    "            # Retrieve and display the execution time\n",
    "            execution_time = list(job.parse_output(runner.cat_output()))[0][1]\n",
    "            print(f\"Input Split Size: {split_size} MB, Execution Time: {execution_time:.4f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
