{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac6545e",
   "metadata": {},
   "source": [
    "DAY-6(Apache Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66871f25",
   "metadata": {},
   "source": [
    "1. Working with RDDs:\n",
    "   a) Write a Python program to create an RDD from a local data source.\n",
    "   b) Implement transformations and actions on the RDD to perform data processing tasks.\n",
    "   c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232d414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##1a.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('LocalDataRDDExample') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create an RDD from a local data source\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Perform operations on the RDD\n",
    "squared_rdd = rdd.map(lambda x: x * x)\n",
    "sum_rdd = squared_rdd.reduce(lambda x, y: x + y)\n",
    "\n",
    "# Print the results\n",
    "print(f'Squared RDD: {squared_rdd.collect()}')\n",
    "print(f'Sum of squared RDD: {sum_rdd}')\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4b681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##1b.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('RDDProcessingExample') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create an RDD from a local data source\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Transformations\n",
    "squared_rdd = rdd.map(lambda x: x * x)\n",
    "filtered_rdd = squared_rdd.filter(lambda x: x > 10)\n",
    "\n",
    "# Actions\n",
    "sum_rdd = squared_rdd.reduce(lambda x, y: x + y)\n",
    "count_rdd = filtered_rdd.count()\n",
    "first_element = filtered_rdd.first()\n",
    "collected_rdd = filtered_rdd.collect()\n",
    "\n",
    "# Print the results\n",
    "print(f'Squared RDD: {squared_rdd.collect()}')\n",
    "print(f'Filtered RDD: {filtered_rdd.collect()}')\n",
    "print(f'Sum of squared RDD: {sum_rdd}')\n",
    "print(f'Count of filtered RDD: {count_rdd}')\n",
    "print(f'First element of filtered RDD: {first_element}')\n",
    "print(f'Collected RDD: {collected_rdd}')\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067918a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##1c.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('RDDDataManipulationExample') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create an RDD from a local data source\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Perform RDD operations\n",
    "squared_rdd = rdd.map(lambda x: x * x)\n",
    "filtered_rdd = squared_rdd.filter(lambda x: x > 10)\n",
    "sum_rdd = squared_rdd.reduce(lambda x, y: x + y)\n",
    "product_rdd = filtered_rdd.reduce(lambda x, y: x * y)\n",
    "mean_rdd = squared_rdd.mean()\n",
    "\n",
    "# Print the results\n",
    "print(f'Squared RDD: {squared_rdd.collect()}')\n",
    "print(f'Filtered RDD: {filtered_rdd.collect()}')\n",
    "print(f'Sum of squared RDD: {sum_rdd}')\n",
    "print(f'Product of filtered RDD: {product_rdd}')\n",
    "print(f'Mean of squared RDD: {mean_rdd}')\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf70190",
   "metadata": {},
   "source": [
    "2. Spark DataFrame Operations:\n",
    "   a) Write a Python program to load a CSV file into a Spark DataFrame.\n",
    "   b)Perform common DataFrame operations such as filtering, grouping, or joining.\n",
    "   c) Apply Spark SQL queries on the DataFrame to extract insights from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75c0b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##2a.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('CSVLoadingExample') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load CSV file into DataFrame\n",
    "csv_file_path = 'path/to/your/csv_file.csv'\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Perform operations on the DataFrame\n",
    "# For example, display the DataFrame schema and the first few rows\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3badcef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##2b.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34514c4",
   "metadata": {},
   "source": [
    "3. Spark Streaming:\n",
    "  a) Write a Python program to create a Spark Streaming application.\n",
    "   b) Configure the application to consume data from a streaming source (e.g., Kafka or a socket).\n",
    "   c) Implement streaming transformations and actions to process and analyze the incoming data stream.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##3a.\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('SparkStreamingExample') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a StreamingContext with a batch interval of 1 second\n",
    "ssc = StreamingContext(spark.sparkContext, 1)\n",
    "\n",
    "# Set the log level to only display error messages\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# Create a DStream by consuming data from a TCP socket\n",
    "lines = ssc.socketTextStream('localhost', 9999)\n",
    "\n",
    "# Perform transformations and actions on the DStream\n",
    "word_counts = lines.flatMap(lambda line: line.split(' ')) \\\n",
    "                   .map(lambda word: (word, 1)) \\\n",
    "                   .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the word counts\n",
    "word_counts.pprint()\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the streaming to finish\n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd855e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##3b.\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('SparkStreamingKafkaExample') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a StreamingContext with a batch interval of 1 second\n",
    "ssc = StreamingContext(spark.sparkContext, 1)\n",
    "\n",
    "# Set the log level to only display error messages\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# Kafka broker details\n",
    "bootstrap_servers = 'localhost:9092'\n",
    "topic = 'my_topic'\n",
    "\n",
    "# Create a Kafka direct stream\n",
    "kafka_stream = KafkaUtils.createDirectStream(ssc, [topic], {'metadata.broker.list': bootstrap_servers})\n",
    "\n",
    "# Get the messages from the Kafka stream\n",
    "messages = kafka_stream.map(lambda x: x[1])\n",
    "\n",
    "# Perform transformations and actions on the messages\n",
    "word_counts = messages.flatMap(lambda line: line.split(' ')) \\\n",
    "                     .map(lambda word: (word, 1)) \\\n",
    "                     .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the word counts\n",
    "word_counts.pprint()\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the streaming to finish\n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed82db",
   "metadata": {},
   "outputs": [],
   "source": [
    "##3c.\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('StreamingDataProcessingExample') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a StreamingContext with a batch interval of 1 second\n",
    "ssc = StreamingContext(spark.sparkContext, 1)\n",
    "\n",
    "# Set the log level to only display error messages\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# Kafka broker details\n",
    "bootstrap_servers = 'localhost:9092'\n",
    "topic = 'my_topic'\n",
    "\n",
    "# Create a Kafka direct stream\n",
    "kafka_stream = KafkaUtils.createDirectStream(ssc, [topic], {'metadata.broker.list': bootstrap_servers})\n",
    "\n",
    "# Get the messages from the Kafka stream\n",
    "messages = kafka_stream.map(lambda x: x[1])\n",
    "\n",
    "# Perform streaming transformations and actions\n",
    "word_counts = messages.flatMap(lambda line: line.split(' ')) \\\n",
    "                     .map(lambda word: (word, 1)) \\\n",
    "                     .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the word counts for each batch interval\n",
    "word_counts.pprint()\n",
    "\n",
    "# Perform windowed word count for the last 10 seconds of data\n",
    "windowed_word_counts = messages.window(10, 5) \\\n",
    "                              .flatMap(lambda line: line.split(' ')) \\\n",
    "                              .map(lambda word: (word, 1)) \\\n",
    "                              .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the windowed word counts\n",
    "windowed_word_counts.pprint()\n",
    "\n",
    "# Perform a stateful word count\n",
    "def update_func(new_values, last_sum):\n",
    "    return sum(new_values) + (last_sum or 0)\n",
    "\n",
    "stateful_word_counts = messages.flatMap(lambda line: line.split(' ')) \\\n",
    "                              .map(lambda word: (word, 1)) \\\n",
    "                              .updateStateByKey(update_func)\n",
    "\n",
    "# Print the stateful word counts\n",
    "stateful_word_counts.pprint()\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the streaming to finish\n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d97a7c",
   "metadata": {},
   "source": [
    "4. Spark SQL and Data Source Integration:\n",
    "   a) Write a Python program to connect Spark with a relational database (e.g., MySQL, PostgreSQL).\n",
    "   b)Perform SQL operations on the data stored in the database using Spark SQL.\n",
    "   c) Explore the integration capabilities of Spark with other data sources, such as Hadoop Distributed File System (HDFS) or Amazon S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2d4304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4c0dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddfd75d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647109b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
